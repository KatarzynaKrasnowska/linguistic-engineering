\documentclass[oneside,12pt]{article}

\usepackage{polski}
\usepackage[utf8]{inputenc}
\usepackage{a4wide}

\title{Extended tokeniser for Polish}
\author{Tomasz Bartosiak \\ Konrad Gołuchowski \\ Katarzyna Krasnowska}

\begin{document}
\maketitle

\section{Method description}

\paragraph{}
The tokenisation (augmented with simple tagging with token type) implemented in our program consists of 4 main steps:

\subsection{Basic splitting}

\paragraph{}
At this stage, the most basic splitting operations are performed on the input text. Each sentence is split on spaces. Additionaly, when the resulting tokens begin or end with interpunction characters, the leading and trailing iterpunction is stripped into separate tokens. This allows, e.g., for separating parentheses, colons and semicolons from neighbouring tokens. An exception from this is the treatment of a dot preceded by a non-interpunction character. Such a dot is kept within the same token for later processing of abbreviations.

\subsection{Filters cascade}

\paragraph{}
In the second stage, the most of the tagging is performed. A~series of token-type filters is defined together with an order in which they are applied, forming what we called a~\textit{filters cascade}. Each filter may either:
    \begin{itemize}
        \item Recognise a~token as belonging to one of the defined types and tag it. In this case, the tagging is done for the given token and the cascade is run on the next token.
        \item Recognise a~token as a~concatenation of proper tokens, split them, tag some of them if possible and leave the rest to be recursively passed through the cascade.
        \item Fail to recognise the token: in this case, the next filter from the cascade is applied.
    \end{itemize}

Therefore, at this stage, token boundaries may be either left as they were determined in the previous stage, or tokens may be further split.
    
\paragraph{}
The simplest filters use regular expressions. In that way, e.g., roman/arabic numerals, e-mail or WWW addresses can be tagged. A little more complicated ones may split the token based on a~regular expression and assign all resulting parts a~tag, as is done, e.g., in the case of arabic numerals followed by a dot.

\subsubsection{Abbreviations}

\paragraph{}
The most complex filter is the one used for recognising abbreviations. It uses some general heuristics and makes use of some predefined list of valid abbreviations of different type, included in \texttt{.txt} files (see the files description at the end of this document) for special cases.

\paragraph{}
The first heuristic marks everything that ends with a~dot (other than a~sentence-ending dot) as abbreviation. Those are eithe non-inflecting abbreviations or inflected forms of some other abbreviation (e.g., \textit{do dr. Pawłowskiego}). In the case of multi-part abbreviations (e.g., \textit{m.in.}) all parts are treated individualy (soe there are 4 tokes in this example: \textit{m}/\texttt{abbrev} \textit{.}/\texttt{punct} \textit{in}/\texttt{abbrev} \textit{.}/\texttt{punct}).

\paragraph{}
The second heuristic marks every token which begins with an upper-case capital letter and has at least one more inside (not counting those which appear after a dash, like in \textit{Austro-Węgry}). This heuristic aims at recognising abbreviations such as \textit{PKO}, \textit{PKiN}. Inflected forms of such abbreviations are also (mostly) recognised with this simple filter, e.g., \textit{w OSiR-ze}.

\paragraph{}
All words not ending with dots are looked up on our abbreviations list. The list includes physical units with optional prefixes (\texttt{unit\_prefixes.txt} + \texttt{unit\_names.txt}), uninflected abbreviations (\texttt{uninflected.txt}) and naively created forms of inflected abbreviations (\texttt{inflect\_base.txt} + \texttt{inflect\_ending.txt}).

\paragraph{}
A dot-ended token at the end of a sentence may also be an abbreviation. It is looked up among the abbreviations not ending with a dot, and those ending with dots (\texttt{dots\_sorted.txt} and \texttt{multi\_part\_dot\_abbr.txt}).

\subsubsection{List of applied filters}

\paragraph{}
Below is the list of filters in the order they are applied:
    \begin{itemize}
        \item Arabic integer numbers are temporarily assigned a~helper tag \texttt{int} (to distinguish them from fractional numbers when parsing dates).
        \item Fractional numbers are assigned the \texttt{ara} tag.
        \item Arabic numbers followed by a dot are split into two tokens and tagged \texttt{ara} and \texttt{punct} respectively.
        \item Upper-case \textit{I} at the beginning of a sentence and lower-case \textit{i} are tagged \texttt{word} (so that they are not passed to the roman numbers filter).
        \item \textit{W}, \textit{A}, \textit{U}, \textit{Z}, \textit{O} at the beginning of a sentence are tagged as \texttt{word} (so that they are not passed to the abbreviations filter).
        \item Roman numbers are assigned the \texttt{rom} tag.
        \item Roman numbers followed by a dot are split into two tokens and tagged \texttt{rom} and \texttt{punct} respectively.
        \item Dates in format \textit{dd.mm.yyyy} and \textit{dd/mm/yyyy} are temporarily assigned a~helper tag \texttt{date}.
        \item Month names in nominative\footnote{Although dates written as \textit{1 kwiecień 2014} are considered incorrect, they often appear in written and spoken Polish, so they are taken into account.} and genitive are temporarily assigned helper tags, e.g., \textit{kwietnia} is tagged as \texttt{m-iv} (for the purpose of date parsing).
        \item Abbreviations and their dots are recognised, split and tagged.
        \item Tokens consisting only of letters are tagged as \texttt{word}.
        \item Interpunction characters are tagged as \texttt{interp}.
        \item A dot-ended token at the end of a sentence is split and tagged.
        \item WWW addresses are tagged as \texttt{www}.
        \item Comma-separated sequences are split and tagged.
        \item Hyphen-separated sequences are split and tagged.
        \item E-mail addresses are tagged as \texttt{e-mail}.
        \item Remaining tokens are tagged \texttt{punct} if they are single characters and \texttt{word} otherwise.
    \end{itemize}

\subsection{Date parsing}

\paragraph{}
Last, dates in various text and number format are recognised based on the processing in the previous stage and appropriate tags are determined, which amounts to parsing the dates and producing their representation in the normalised format \texttt{yyyy.mm.dd}. At this stage, several tokens may be joined back together to form a single one.

\paragraph{}
In the simplest case, day, month and year are retrieved from tokens recognised as \texttt{date} by the filter cascade. Other date formats are recognised as specific token patterns, e.g., tag=\texttt{int} - tag=\texttt{m-$\ast$} - tag=\texttt{int} - tok=\texttt{"r"} - tok=\texttt{"."} (this matches, for instance, \textit{14 marca 2014 r.}).

\subsection{Clean-up}

\paragraph{}
To complete the extended tokenisation process, the helper tags \texttt{int} and \texttt{m-$\ast$} tags are replaced with \texttt{ara} and \texttt{word} respectively. As the last small step, \textit{I} and \textit{i} tokens followed by a closing parenthesis \textit{)} are assigned the \texttt{rom} tag (those are most likely items on a~numbered list, but the heuristic for distinguishing between conjuction and roman 1 applied at the filter stage did not look at the next token and tagged them as \texttt{word}).

\section{Authors' contributions}

\paragraph{Tomasz Bartosiak:} handling XML format of input/output files, abbreviations.

\paragraph{Konrad Gołuchowski:} filters cascade stage: project and particular filters.

\paragraph{Katarzyna Krasnowska:} filters cascade stage: particular filters, dates.

\paragraph{}
Besides the above, each author provided some testing input files, repeatedly tested the method against those files and fixed or reported detected problems. 

\section{Files description}

\paragraph{}
Python source code:
\begin{itemize}
    \item \texttt{main.py} -- the main program file, contains high-level code for handling input/output files and code for first-stage tokenisation.
    
    \item \texttt{token.py} -- contains filters used in the filters cascade stage.
    
    \item \texttt{date.py} -- contains code for handling dates.
    
    \item \texttt{tags.py} -- tag names defined as constants for convenience.
    
    \item \texttt{ext\_tokenizer\_xml\_parsing.py} -- contains code for parsing and printing XML files.
\end{itemize}

\paragraph{}
Other files used by the program:
\begin{itemize}
    \item \texttt{dots\_sorted.txt} -- list of abbreviations ending with dot.
    
    \item \texttt{multi\_part\_dot\_abbr.txt} -- list of multi-part abbreviations ending with dot.
    
    \item \texttt{unit\_names.txt} -- list of abbreviations for physical units.
    
    \item \texttt{unit\_prefixes.txt} -- list of prefixes for physical units.
    
    \item \texttt{uninflected.txt} -- list of uninflected abbreviations not ending with dot
    
    \item \texttt{inflect\_base.txt} -- list of stems for inflected abbreviations.
    
    \item \texttt{inflect\_ending.txt} -- list of possible endings for inflected forms of abbreviations.
\end{itemize}

\end{document}
